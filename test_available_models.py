# è»½é‡ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèª
import requests
import json
import time

def test_available_models():
    """åˆ©ç”¨å¯èƒ½ãªè»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("ğŸš€ åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«å‹•ä½œç¢ºèª")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é †ï¼ˆè»½é‡â†’é‡é‡ï¼‰
    models_to_test = [
        ("codellama:7b", "3.8GB"), 
        ("starcoder:7b", "4.3GB"),
        ("phi4:14b", "9.1GB"),
        ("wizardcoder:33b", "18GB")
    ]
    
    working_models = []
    
    for model, size in models_to_test:
        print(f"\nğŸ“ {model} ({size}) ãƒ†ã‚¹ãƒˆ:")
        
        try:
            payload = {
                "model": model,
                "prompt": "def fibonacci(n):",
                "stream": False,
                "options": {
                    "num_predict": 100,
                    "temperature": 0.1
                }
            }
            
            start_time = time.time()
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=120
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get('response', '')
                print(f"âœ… {model} æˆåŠŸ! (å¿œç­”æ™‚é–“: {end_time - start_time:.1f}ç§’)")
                print(f"ğŸ“„ å¿œç­”: {response_text[:150]}...")
                working_models.append({
                    'model': model,
                    'size': size,
                    'response_time': end_time - start_time,
                    'response_length': len(response_text)
                })
            else:
                print(f"âŒ {model} ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                if response.text:
                    error_detail = response.text[:200]
                    print(f"è©³ç´°: {error_detail}")
                
        except requests.exceptions.Timeout:
            print(f"â±ï¸ {model} ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (120ç§’)")
        except Exception as e:
            print(f"âŒ {model} ä¾‹å¤–: {e}")
        
        # ãƒ¢ãƒ‡ãƒ«é–“ã§å°‘ã—å¾…æ©Ÿ
        time.sleep(2)
    
    return working_models

if __name__ == "__main__":
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«ã‚ˆã‚Šã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™")
    working_models = test_available_models()
    
    if working_models:
        print(f"\nğŸ‰ å‹•ä½œç¢ºèªæ¸ˆã¿ãƒ¢ãƒ‡ãƒ« ({len(working_models)}å€‹):")
        for model_info in working_models:
            print(f"  âœ… {model_info['model']} ({model_info['size']}) - {model_info['response_time']:.1f}ç§’")
        
        # æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨
        fastest_model = min(working_models, key=lambda x: x['response_time'])
        print(f"\nğŸ’¡ æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {fastest_model['model']} (æœ€é€Ÿå¿œç­”: {fastest_model['response_time']:.1f}ç§’)")
        
    else:
        print("\nâŒ å‹•ä½œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
