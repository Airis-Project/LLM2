# å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# python test_llm_real.py

# -*- coding: utf-8 -*-
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.llm_client_v2 import EnhancedLLMClient
from src.model_selector import TaskType
import time

def test_real_llm_calls():
    """å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        client = EnhancedLLMClient()
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ç¢ºèª
        available_models = client.get_available_models()
        print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {len(available_models)}å€‹")
        for model in available_models[:5]:  # æœ€åˆã®5å€‹ã ã‘è¡¨ç¤º
            print(f"   â€¢ {model}")
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆçŸ­æ™‚é–“ã§å®Œäº†ã™ã‚‹ã‚‚ã®ï¼‰
        test_cases = [
            {
                "name": "é«˜é€Ÿã‚³ãƒ¼ãƒ‰è£œå®Œ",
                "task": TaskType.QUICK_RESPONSE,
                "priority": "speed",
                "prompt": "def fibonacci(n):",
                "max_tokens": 150
            },
            {
                "name": "ç°¡å˜ãªã‚³ãƒ¼ãƒ‰èª¬æ˜",
                "task": TaskType.CODE_EXPLANATION,
                "priority": "quality",
                "prompt": "Explain this code: x = [i**2 for i in range(5)]",
                "max_tokens": 200
            },
            {
                "name": "ã‚³ãƒ¼ãƒ‰ãƒ‡ãƒãƒƒã‚°",
                "task": TaskType.DEBUGGING,
                "priority": "balanced",
                "prompt": "Fix this code: def add(a, b) return a + b",
                "max_tokens": 100
            }
        ]
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆ {i}: {test_case['name']}")
            print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {test_case['prompt']}")
            
            start_time = time.time()
            
            try:
                result = client.generate_code(
                    prompt=test_case['prompt'],
                    task_type=test_case['task'],
                    priority=test_case['priority'],
                    num_predict=test_case['max_tokens']
                )
                
                end_time = time.time()
                actual_time = end_time - start_time
                
                if result['success']:
                    response_text = result['response'].strip()
                    print(f"   âœ… æˆåŠŸ ({actual_time:.1f}s)")
                    print(f"   ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {result['model']}")
                    print(f"   ğŸ“ å¿œç­”: {response_text[:150]}...")
                    
                    results.append({
                        "test": test_case['name'],
                        "success": True,
                        "time": actual_time,
                        "model": result['model'],
                        "response_length": len(response_text)
                    })
                else:
                    print(f"   âŒ å¤±æ•—: {result.get('error', 'Unknown error')}")
                    results.append({
                        "test": test_case['name'],
                        "success": False,
                        "time": actual_time,
                        "error": result.get('error', 'Unknown error')
                    })
                    
            except Exception as e:
                end_time = time.time()
                actual_time = end_time - start_time
                print(f"   ğŸ’¥ ä¾‹å¤– ({actual_time:.1f}s): {e}")
                results.append({
                    "test": test_case['name'],
                    "success": False,
                    "time": actual_time,
                    "error": str(e)
                })
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        successful_tests = sum(1 for r in results if r['success'])
        total_time = sum(r['time'] for r in results)
        
        print(f"   æˆåŠŸç‡: {successful_tests}/{len(results)} ({successful_tests/len(results):.1%})")
        print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        
        for result in results:
            status = "âœ…" if result['success'] else "âŒ"
            if result['success']:
                print(f"   {status} {result['test']}: {result['time']:.1f}s ({result['model']})")
            else:
                print(f"   {status} {result['test']}: {result.get('error', 'Failed')}")
        
        # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ“ˆ æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ:")
        try:
            perf_report = client.get_performance_report()
            print(f"   ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {perf_report['total_requests']}")
            print(f"   æˆåŠŸç‡: {perf_report['success_rate']}")
            print(f"   å¹³å‡å¿œç­”æ™‚é–“: {perf_report['avg_response_time']}")
            
            print(f"   ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çŠ¶æ³:")
            for model, count in perf_report.get('model_usage', {}).items():
                print(f"     â€¢ {model}: {count}å›")
                
            if perf_report.get('error_summary'):
                print(f"   ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼:")
                for error, count in perf_report['error_summary'].items():
                    print(f"     â€¢ {error}: {count}å›")
                    
        except Exception as e:
            print(f"   æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_real_llm_calls()