# test_local_llm.py
from src.llm.llm_factory import LLMFactory
from src.core.config_manager import ConfigManager

def test_local_llm_integration():
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±åˆãƒ†ã‚¹ãƒˆ"""
    print("ğŸ”§ LLMFactory + Ollama çµ±åˆãƒ†ã‚¹ãƒˆ")
    
    try:
        # è¨­å®šç¢ºèª
        config = ConfigManager()
        llm_config = config.get('llm', {})
        print(f"è¨­å®šç¢ºèª: {llm_config}")
        
        # LLMFactoryåˆæœŸåŒ–
        factory = LLMFactory()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å–å¾—
        local_llm = factory.get_provider('local')
        print(f"âœ… ãƒ­ãƒ¼ã‚«ãƒ«LLMå–å¾—æˆåŠŸ: {type(local_llm)}")
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆ
        response = local_llm.generate("def hello():")
        print(f"âœ… ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ†ã‚¹ãƒˆæˆåŠŸ: {response[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    test_local_llm_integration()
