# æœ€çµ‚ç‰ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
#python test_integration_final.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆ - æœ€çµ‚ä¿®æ­£ç‰ˆï¼ˆgenerate_codeä½¿ç”¨ï¼‰
"""

import sys
import time
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent))

from src.llm_client_v2 import EnhancedLLMClient
from src.model_selector import TaskType

def print_section(title: str):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {title}")
    print('='*60)

def print_test(name: str, prompt: str):
    """ãƒ†ã‚¹ãƒˆé–‹å§‹è¡¨ç¤º"""
    print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ: {name}")
    print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt[:50]}{'...' if len(prompt) > 50 else ''}")

def run_comprehensive_tests():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆgenerate_codeä½¿ç”¨ï¼‰"""
    
    print_section("LLM Code Assistant å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆæœ€çµ‚ç‰ˆï¼‰")
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = EnhancedLLMClient()
    
    print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {len(client.get_available_models())}å€‹")
    for model in client.get_available_models()[:5]:
        print(f"   â€¢ {model}")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
    test_cases = [
        # åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        {
            "name": "Pythoné–¢æ•°ç”Ÿæˆ",
            "prompt": "Create a simple Python function to calculate factorial",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "balanced"
        },
        {
            "name": "JavaScripté–¢æ•°ç”Ÿæˆ", 
            "prompt": "Write a simple JavaScript function to validate email",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "balanced"
        },
        {
            "name": "SQL ã‚¯ã‚¨ãƒªç”Ÿæˆ",
            "prompt": "Write SQL to select top 5 users",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "balanced"
        },
        
        # ã‚³ãƒ¼ãƒ‰èª¬æ˜ãƒ†ã‚¹ãƒˆ
        {
            "name": "ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜èª¬æ˜",
            "prompt": "Explain this code: squares = [x**2 for x in range(10)]",
            "task_type": TaskType.CODE_EXPLANATION,
            "priority": "balanced"
        },
        {
            "name": "é–¢æ•°èª¬æ˜",
            "prompt": "Explain this function: def greet(name): return f'Hello, {name}!'",
            "task_type": TaskType.CODE_EXPLANATION,
            "priority": "balanced"
        },
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¹ãƒˆ
        {
            "name": "Pythonæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "prompt": "Fix this Python code: for i in range(5) print(i)",
            "task_type": TaskType.DEBUGGING,
            "priority": "speed"
        },
        {
            "name": "è«–ç†ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "prompt": "Fix this code: def is_odd(n): return n % 2 == 0",
            "task_type": TaskType.DEBUGGING,
            "priority": "balanced"
        },
        
        # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚³ãƒ¼ãƒ‰æœ€é©åŒ–",
            "prompt": "Improve this code: def sum_list(lst): total = 0; for item in lst: total += item; return total",
            "task_type": TaskType.REFACTORING,
            "priority": "balanced"
        },
        
        # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚¯ã‚¤ãƒƒã‚¯è£œå®Œ1",
            "prompt": "Complete: def hello():",
            "task_type": TaskType.QUICK_RESPONSE,
            "priority": "speed"
        },
        {
            "name": "ã‚¯ã‚¤ãƒƒã‚¯è£œå®Œ2", 
            "prompt": "Complete: import os; path = os.path.",
            "task_type": TaskType.QUICK_RESPONSE,
            "priority": "speed"
        },
        
        # è¤‡é›‘åˆ†æãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æ",
            "prompt": "Analyze bubble sort time complexity",
            "task_type": TaskType.COMPLEX_ANALYSIS,
            "priority": "quality"
        },
        {
            "name": "è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³èª¬æ˜",
            "prompt": "Explain Factory pattern briefly",
            "task_type": TaskType.COMPLEX_ANALYSIS,
            "priority": "quality"
        }
    ]
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = []
    total_start_time = time.time()
    
    for i, test_case in enumerate(test_cases, 1):
        print_test(f"{i}. {test_case['name']}", test_case['prompt'])
        
        start_time = time.time()
        try:
            # æ­£ã—ã„ãƒ¡ã‚½ãƒƒãƒ‰åï¼šgenerate_code ã‚’ä½¿ç”¨
            response = client.generate_code(
                prompt=test_case['prompt'],
                task_type=test_case['task_type'],
                priority=test_case['priority']
            )
            
            duration = time.time() - start_time
            
            if response['success']:
                print(f"   âœ… æˆåŠŸ ({duration:.1f}s)")
                print(f"   ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {response['model']}")
                print(f"   ğŸ“ å¿œç­”: {response['response'][:100]}{'...' if len(response['response']) > 100 else ''}")
                
                results.append({
                    'name': test_case['name'],
                    'success': True,
                    'duration': duration,
                    'model': response['model'],
                    'task_type': test_case['task_type'].value,
                    'priority': test_case['priority'],
                    'content_length': len(response['response']),
                    'response_time': response.get('response_time', duration)
                })
            else:
                print(f"   âŒ å¤±æ•—: {response['error']}")
                results.append({
                    'name': test_case['name'],
                    'success': False,
                    'duration': duration,
                    'error': response['error'],
                    'task_type': test_case['task_type'].value,
                    'priority': test_case['priority'],
                    'model': response.get('model', 'unknown')
                })
                
        except Exception as e:
            duration = time.time() - start_time
            print(f"   âŒ ä¾‹å¤–: {e}")
            results.append({
                'name': test_case['name'],
                'success': False,
                'duration': duration,
                'error': str(e),
                'task_type': test_case['task_type'].value,
                'priority': test_case['priority']
            })
    
    # çµæœåˆ†æ
    total_duration = time.time() - total_start_time
    successful_tests = [r for r in results if r['success']]
    failed_tests = [r for r in results if not r['success']]
    
    print_section("å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆçµæœ")
    
    print(f"ğŸ“Š ç·åˆçµæœ:")
    print(f"   æˆåŠŸç‡: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.1f}%)")
    print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.1f}ç§’")
    
    if successful_tests:
        avg_duration = sum(r['duration'] for r in successful_tests) / len(successful_tests)
        avg_content_length = sum(r['content_length'] for r in successful_tests) / len(successful_tests)
        avg_response_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)
        print(f"   å¹³å‡å¿œç­”æ™‚é–“: {avg_duration:.1f}ç§’")
        print(f"   å¹³å‡LLMå¿œç­”æ™‚é–“: {avg_response_time:.1f}ç§’")
        print(f"   å¹³å‡å¿œç­”é•·: {avg_content_length:.0f}æ–‡å­—")
    else:
        print("   å¹³å‡å¿œç­”æ™‚é–“: N/A")
    
    # æˆåŠŸãƒ†ã‚¹ãƒˆè©³ç´°
    if successful_tests:
        print(f"\nâœ… æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ ({len(successful_tests)}å€‹):")
        for result in successful_tests:
            print(f"   â€¢ {result['name']}: {result['duration']:.1f}s ({result['model']}) - {result['content_length']}æ–‡å­—")
    
    # å¤±æ•—ãƒ†ã‚¹ãƒˆè©³ç´°
    if failed_tests:
        print(f"\nâŒ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ ({len(failed_tests)}å€‹):")
        for result in failed_tests:
            print(f"   â€¢ {result['name']}: {result.get('error', 'Unknown error')}")
    
    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
    print(f"\nğŸ“ˆ ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥æˆåŠŸç‡:")
    task_types = {}
    for result in results:
        task_type = result['task_type']
        if task_type not in task_types:
            task_types[task_type] = {'total': 0, 'success': 0, 'avg_time': []}
        task_types[task_type]['total'] += 1
        if result['success']:
            task_types[task_type]['success'] += 1
            task_types[task_type]['avg_time'].append(result['duration'])
    
    for task_type, stats in task_types.items():
        success_rate = stats['success'] / stats['total'] * 100
        avg_time = sum(stats['avg_time']) / len(stats['avg_time']) if stats['avg_time'] else 0
        print(f"   â€¢ {task_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%) - å¹³å‡{avg_time:.1f}s")
    
    # å„ªå…ˆåº¦åˆ¥åˆ†æ
    print(f"\nâš¡ å„ªå…ˆåº¦åˆ¥å¹³å‡å¿œç­”æ™‚é–“:")
    priorities = {}
    for result in successful_tests:
        priority = result['priority']
        if priority not in priorities:
            priorities[priority] = []
        priorities[priority].append(result['duration'])
    
    for priority, durations in priorities.items():
        if durations:
            avg_duration = sum(durations) / len(durations)
            print(f"   â€¢ {priority}: {avg_duration:.1f}ç§’ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(durations)})")
    
    # ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çµ±è¨ˆ
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çµ±è¨ˆ:")
    model_usage = {}
    total_chars = {}
    for result in successful_tests:
        model = result['model']
        model_usage[model] = model_usage.get(model, 0) + 1
        total_chars[model] = total_chars.get(model, 0) + result['content_length']
    
    for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):
        avg_chars = total_chars[model] / count if count > 0 else 0
        print(f"   â€¢ {model}: {count}å› (å¹³å‡{avg_chars:.0f}æ–‡å­—/å›)")
    
    # æ€§èƒ½è©•ä¾¡
    print(f"\nğŸ† æ€§èƒ½è©•ä¾¡:")
    if successful_tests:
        fastest_test = min(successful_tests, key=lambda x: x['duration'])
        slowest_test = max(successful_tests, key=lambda x: x['duration'])
        longest_response = max(successful_tests, key=lambda x: x['content_length'])
        
        print(f"   â€¢ æœ€é€Ÿãƒ†ã‚¹ãƒˆ: {fastest_test['name']} ({fastest_test['duration']:.1f}s)")
        print(f"   â€¢ æœ€é…ãƒ†ã‚¹ãƒˆ: {slowest_test['name']} ({slowest_test['duration']:.1f}s)")
        print(f"   â€¢ æœ€é•·å¿œç­”: {longest_response['name']} ({longest_response['content_length']}æ–‡å­—)")
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ:")
    perf_report = client.get_performance_report()
    for key, value in perf_report.items():
        print(f"   â€¢ {key}: {value}")
    
    print_section("çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
    
    return {
        'total_tests': len(results),
        'successful_tests': len(successful_tests),
        'success_rate': len(successful_tests) / len(results) * 100,
        'total_duration': total_duration,
        'results': results,
        'performance_report': perf_report
    }

if __name__ == "__main__":
    # å®Ÿè¡Œ
    final_results = run_comprehensive_tests()
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print(f"\nğŸ¯ æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼:")
    print(f"   â€¢ ç·ãƒ†ã‚¹ãƒˆæ•°: {final_results['total_tests']}")
    print(f"   â€¢ æˆåŠŸæ•°: {final_results['successful_tests']}")
    print(f"   â€¢ æˆåŠŸç‡: {final_results['success_rate']:.1f}%")
    print(f"   â€¢ ç·å®Ÿè¡Œæ™‚é–“: {final_results['total_duration']:.1f}ç§’")

