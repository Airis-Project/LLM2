# src/llm/local_llm_client.py
"""
ローカルLLMクライアントモジュール
Ollama、llama.cpp等のローカル実行LLMとの統合
"""

import requests
import json
import aiohttp
import asyncio
from typing import Iterator, Dict, Any, Optional, List, AsyncIterator
from .base_llm import BaseLLM, LLMConfig, LLMRole
from ..core.logger import get_logger

class LocalLLMClient(BaseLLM):
    """ローカルLLMクライアント（Ollama対応）"""
    
    def __init__(self, config: LLMConfig):
        """
        初期化
        
        Args:
            config: LLM設定
        """
        super().__init__(config)
        self.logger = get_logger(self.__class__.__name__)
        
        # 設定から値を取得
        self.model_name = config.model
        self.backend = getattr(config, 'backend', 'ollama')
        self.base_url = getattr(config, 'base_url', 'http://localhost:11434')
        
        # APIエンドポイントの設定
        if self.backend == 'ollama':
            self.api_url = f"{self.base_url}/api"
            self.generate_endpoint = f"{self.api_url}/generate"
            self.chat_endpoint = f"{self.api_url}/chat"
            self.models_endpoint = f"{self.api_url}/tags"
        else:
            # 他のバックエンド（llama.cpp等）の場合
            self.api_url = self.base_url
            self.generate_endpoint = f"{self.api_url}/completion"
            self.chat_endpoint = f"{self.api_url}/v1/chat/completions"
            self.models_endpoint = f"{self.api_url}/v1/models"
        
        self.logger.info(f"ローカルLLMクライアントを初期化: {self.backend} @ {self.base_url}")
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        チャット実行（一括）
        
        Args:
            messages: メッセージリスト
            **kwargs: 追加パラメータ
            
        Returns:
            str: 応答文字列
        """
        try:
            response_parts = []
            for chunk in self.chat_stream(messages, **kwargs):
                response_parts.append(chunk)
            return ''.join(response_parts)
        except Exception as e:
            self.logger.error(f"ローカルLLMチャットエラー: {e}")
            raise Exception(f"ローカルLLMチャットエラー: {str(e)}")
    
    def chat_stream(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[str]:
        """
        ストリーミングチャット
        
        Args:
            messages: メッセージリスト
            **kwargs: 追加パラメータ
            
        Yields:
            str: 応答チャンク
        """
        try:
            if self.backend == 'ollama':
                yield from self._ollama_chat_stream(messages, **kwargs)
            else:
                yield from self._generic_chat_stream(messages, **kwargs)
                
        except Exception as e:
            self.logger.error(f"ローカルLLMストリーミングエラー: {e}")
            raise Exception(f"ローカルLLMストリーミングエラー: {str(e)}")
    
    def _ollama_chat_stream(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[str]:
        """Ollama用ストリーミングチャット"""
        # Ollamaのchat APIを使用（新しいバージョン）
        try:
            payload = {
                "model": self.model_name,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": kwargs.get('temperature', self.config.temperature),
                    "num_predict": kwargs.get('max_tokens', self.config.max_tokens)
                }
            }
            
            response = requests.post(
                self.chat_endpoint,
                json=payload,
                stream=True,
                timeout=30
            )
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode('utf-8'))
                        if 'message' in data and 'content' in data['message']:
                            yield data['message']['content']
                        if data.get('done', False):
                            break
                    except json.JSONDecodeError:
                        continue
                        
        except requests.exceptions.RequestException:
            # chat APIが利用できない場合はgenerate APIにフォールバック
            yield from self._ollama_generate_stream(messages, **kwargs)
    
    def _ollama_generate_stream(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[str]:
        """Ollama generate API用ストリーミング"""
        prompt = self._convert_messages_to_prompt(messages)
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": kwargs.get('temperature', self.config.temperature),
                "num_predict": kwargs.get('max_tokens', self.config.max_tokens)
            }
        }
        
        response = requests.post(
            self.generate_endpoint,
            json=payload,
            stream=True,
            timeout=30
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode('utf-8'))
                    if 'response' in data:
                        yield data['response']
                    if data.get('done', False):
                        break
                except json.JSONDecodeError:
                    continue
    
    def _generic_chat_stream(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[str]:
        """汎用的なチャットAPI用ストリーミング"""
        payload = {
            "messages": messages,
            "model": self.model_name,
            "stream": True,
            "temperature": kwargs.get('temperature', self.config.temperature),
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens)
        }
        
        response = requests.post(
            self.chat_endpoint,
            json=payload,
            stream=True,
            timeout=30
        )
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                line_str = line.decode('utf-8')
                if line_str.startswith('data: '):
                    try:
                        data = json.loads(line_str[6:])
                        if 'choices' in data and len(data['choices']) > 0:
                            delta = data['choices'][0].get('delta', {})
                            if 'content' in delta:
                                yield delta['content']
                    except json.JSONDecodeError:
                        continue
    
    async def chat_async(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        非同期チャット
        
        Args:
            messages: メッセージリスト
            **kwargs: 追加パラメータ
            
        Returns:
            str: 応答文字列
        """
        try:
            response_parts = []
            async for chunk in self.chat_stream_async(messages, **kwargs):
                response_parts.append(chunk)
            return ''.join(response_parts)
        except Exception as e:
            self.logger.error(f"ローカルLLM非同期チャットエラー: {e}")
            raise Exception(f"ローカルLLM非同期チャットエラー: {str(e)}")
    
    async def chat_stream_async(self, messages: List[Dict[str, str]], **kwargs) -> AsyncIterator[str]:
        """
        非同期ストリーミングチャット
        
        Args:
            messages: メッセージリスト
            **kwargs: 追加パラメータ
            
        Yields:
            str: 応答チャンク
        """
        try:
            if self.backend == 'ollama':
                async for chunk in self._ollama_chat_stream_async(messages, **kwargs):
                    yield chunk
            else:
                async for chunk in self._generic_chat_stream_async(messages, **kwargs):
                    yield chunk
                    
        except Exception as e:
            self.logger.error(f"ローカルLLM非同期ストリーミングエラー: {e}")
            raise Exception(f"ローカルLLM非同期ストリーミングエラー: {str(e)}")
    
    async def _ollama_chat_stream_async(self, messages: List[Dict[str, str]], **kwargs) -> AsyncIterator[str]:
        """非同期Ollama用ストリーミングチャット"""
        payload = {
            "model": self.model_name,
            "messages": messages,
            "stream": True,
            "options": {
                "temperature": kwargs.get('temperature', self.config.temperature),
                "num_predict": kwargs.get('max_tokens', self.config.max_tokens)
            }
        }
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(self.chat_endpoint, json=payload) as response:
                    response.raise_for_status()
                    async for line in response.content:
                        if line:
                            try:
                                data = json.loads(line.decode('utf-8'))
                                if 'message' in data and 'content' in data['message']:
                                    yield data['message']['content']
                                if data.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
            except aiohttp.ClientError:
                # chat APIが利用できない場合はgenerate APIにフォールバック
                async for chunk in self._ollama_generate_stream_async(messages, **kwargs):
                    yield chunk
    
    async def _ollama_generate_stream_async(self, messages: List[Dict[str, str]], **kwargs) -> AsyncIterator[str]:
        """非同期Ollama generate API用ストリーミング"""
        prompt = self._convert_messages_to_prompt(messages)
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": kwargs.get('temperature', self.config.temperature),
                "num_predict": kwargs.get('max_tokens', self.config.max_tokens)
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.generate_endpoint, json=payload) as response:
                response.raise_for_status()
                async for line in response.content:
                    if line:
                        try:
                            data = json.loads(line.decode('utf-8'))
                            if 'response' in data:
                                yield data['response']
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
    
    async def _generic_chat_stream_async(self, messages: List[Dict[str, str]], **kwargs) -> AsyncIterator[str]:
        """非同期汎用チャットAPI用ストリーミング"""
        payload = {
            "messages": messages,
            "model": self.model_name,
            "stream": True,
            "temperature": kwargs.get('temperature', self.config.temperature),
            "max_tokens": kwargs.get('max_tokens', self.config.max_tokens)
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.chat_endpoint, json=payload) as response:
                response.raise_for_status()
                async for line in response.content:
                    if line:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            try:
                                data = json.loads(line_str[6:])
                                if 'choices' in data and len(data['choices']) > 0:
                                    delta = data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        yield delta['content']
                            except json.JSONDecodeError:
                                continue
    
    def _convert_messages_to_prompt(self, messages: List[Dict[str, str]]) -> str:
        """メッセージをプロンプト形式に変換"""
        prompt_parts = []
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            
            if role == 'system':
                prompt_parts.append(f"System: {content}")
            elif role == 'user':
                prompt_parts.append(f"Human: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"Assistant: {content}")
        
        prompt_parts.append("Assistant:")
        return "\n\n".join(prompt_parts)
    
    def is_available(self) -> bool:
        """ローカルLLMサーバーの可用性チェック"""
        try:
            response = requests.get(self.models_endpoint, timeout=5)
            return response.status_code == 200
        except Exception:
            return False
    
    def get_available_models(self) -> List[str]:
        """利用可能なモデル一覧を取得"""
        try:
            response = requests.get(self.models_endpoint, timeout=5)
            if response.status_code == 200:
                data = response.json()
                
                if self.backend == 'ollama':
                    return [model['name'] for model in data.get('models', [])]
                else:
                    # OpenAI互換API形式
                    return [model['id'] for model in data.get('data', [])]
            return []
        except Exception:
            return []
    
    def cleanup(self):
        """クリーンアップ"""
        # 必要に応じてリソースの解放
        self.logger.info("ローカルLLMクライアントをクリーンアップしました")
