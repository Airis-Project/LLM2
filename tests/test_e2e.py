# tests/test_e2e.py
"""
End-to-End Tests for LLM Chat System
ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
"""

import os
import sys
import json
import time
import subprocess
import tempfile
import threading
from pathlib import Path
from unittest.mock import patch, Mock
from io import StringIO

import pytest

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


@pytest.mark.integration
@pytest.mark.slow
class TestEndToEnd:
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_system_startup_and_shutdown(self, temp_dir):
        """ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ãƒ»çµ‚äº†ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        config = {
            "version": "1.0.0",
            "application": {"name": "E2E Test", "debug": True},
            "llm": {
                "default_provider": "openai",
                "providers": {
                    "openai": {
                        "enabled": True,
                        "api_key": "test_key",
                        "models": {
                            "default": "gpt-3.5-turbo",
                            "available": ["gpt-3.5-turbo"]
                        }
                    }
                }
            }
        }
        
        config_path = temp_dir / "e2e_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f)
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
        init_result = subprocess.run([
            sys.executable, "scripts/init.py", "--force"
        ], capture_output=True, text=True, cwd=project_root)
        
        assert init_result.returncode == 0, f"Init failed: {init_result.stderr}"
        
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        health_result = subprocess.run([
            sys.executable, "scripts/start.py", "--health-check"
        ], capture_output=True, text=True, cwd=project_root)
        
        assert health_result.returncode == 0, f"Health check failed: {health_result.stderr}"
    
    def test_cli_interactive_session(self, temp_dir):
        """CLIã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        config = {
            "version": "1.0.0",
            "application": {"name": "CLI E2E Test"},
            "llm": {
                "default_provider": "openai",
                "providers": {
                    "openai": {
                        "enabled": True,
                        "api_key": "test_key",
                        "models": {
                            "default": "gpt-3.5-turbo",
                            "available": ["gpt-3.5-turbo"]
                        }
                    }
                }
            }
        }
        
        config_path = temp_dir / "cli_e2e_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f)
        
        # CLIã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆå…¥åŠ›
        test_commands = [
            "help",
            "config",
            "provider",
            "clear",
            "quit"
        ]
        
        # ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•ã¨ã‚³ãƒãƒ³ãƒ‰é€ä¿¡
        process = subprocess.Popen([
            sys.executable, "main.py", 
            "--config", str(config_path),
            "--interface", "cli"
        ], 
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        cwd=project_root
        )
        
        # ã‚³ãƒãƒ³ãƒ‰é€ä¿¡
        input_text = "\n".join(test_commands)
        try:
            stdout, stderr = process.communicate(input=input_text, timeout=30)
            
            # åŸºæœ¬çš„ãªå‡ºåŠ›ç¢ºèª
            assert "LLM Chat System" in stdout
            assert process.returncode == 0
            
        except subprocess.TimeoutExpired:
            process.kill()
            pytest.fail("CLI session timed out")
    
    @pytest.mark.requires_api_key
    def test_real_api_integration(self):
        """å®Ÿéš›ã®APIçµ±åˆãƒ†ã‚¹ãƒˆï¼ˆAPIã‚­ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿ï¼‰"""
        if not os.getenv('OPENAI_API_KEY'):
            pytest.skip("OPENAI_API_KEY not available")
        
        # å®Ÿéš›ã®APIã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚¹ãƒˆ
        from src.core.config_manager import ConfigManager
        from src.core.logger import setup_logger
        from src.services.llm_service import LLMService
        
        # å®Ÿéš›ã®è¨­å®šã§LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
        config_manager = ConfigManager()
        config_manager.load_config("config/default_config.json")
        
        logger = setup_logger("RealAPITest")
        llm_service = LLMService(config_manager, logger)
        
        # ç°¡å˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ãƒ†ã‚¹ãƒˆ
        try:
            response = llm_service.send_message("Hello, this is a test message.")
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼
            assert isinstance(response, str)
            assert len(response) > 0
            assert response.strip() != ""
            
        except Exception as e:
            pytest.fail(f"Real API integration failed: {e}")
    
    def test_configuration_scenarios(self, temp_dir):
        """è¨­å®šã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        scenarios = [
            {
                "name": "minimal_config",
                "config": {
                    "version": "1.0.0",
                    "application": {"name": "Minimal Test"},
                    "llm": {
                        "default_provider": "openai",
                        "providers": {
                            "openai": {
                                "enabled": True,
                                "api_key": "test_key",
                                "models": {
                                    "default": "gpt-3.5-turbo",
                                    "available": ["gpt-3.5-turbo"]
                                }
                            }
                        }
                    }
                }
            },
            {
                "name": "full_config",
                "config": {
                    "version": "1.0.0",
                    "application": {
                        "name": "Full Test",
                        "debug": True,
                        "log_level": "DEBUG"
                    },
                    "llm": {
                        "default_provider": "openai",
                        "timeout": 30,
                        "max_retries": 3,
                        "providers": {
                            "openai": {
                                "enabled": True,
                                "api_key": "test_key",
                                "models": {
                                    "default": "gpt-3.5-turbo",
                                    "available": ["gpt-3.5-turbo", "gpt-4"]
                                },
                                "parameters": {
                                    "temperature": 0.7,
                                    "max_tokens": 2000
                                }
                            }
                        }
                    },
                    "ui": {
                        "default_interface": "cli",
                        "cli": {
                            "prompt_style": "colorful",
                            "show_timestamps": True
                        }
                    },
                    "logging": {
                        "level": "DEBUG",
                        "console_output": True,
                        "file_output": True
                    }
                }
            }
        ]
        
        for scenario in scenarios:
            config_path = temp_dir / f"{scenario['name']}_config.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(scenario['config'], f)
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ãƒ†ã‚¹ãƒˆ
            result = subprocess.run([
                sys.executable, "scripts/start.py",
                "--config", str(config_path),
                "--health-check"
            ], capture_output=True, text=True, cwd=project_root)
            
            assert result.returncode == 0, \
                f"Scenario '{scenario['name']}' failed: {result.stderr}"
    
    def test_error_recovery_scenarios(self, temp_dir):
        """ã‚¨ãƒ©ãƒ¼å›å¾©ã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        # ç„¡åŠ¹ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        invalid_config = {
            "version": "invalid",
            "application": {},
            "llm": {"providers": {}}
            }
        
        config_path = temp_dir / "invalid_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(invalid_config, f)
        
        # ç„¡åŠ¹ãªè¨­å®šã§ã®èµ·å‹•ãƒ†ã‚¹ãƒˆï¼ˆå¤±æ•—ã™ã‚‹ã“ã¨ã‚’æœŸå¾…ï¼‰
        result = subprocess.run([
            sys.executable, "scripts/start.py",
            "--config", str(config_path)
        ], capture_output=True, text=True, cwd=project_root)
        
        # ã‚¨ãƒ©ãƒ¼ã§çµ‚äº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert result.returncode != 0
        assert "error" in result.stderr.lower() or "failed" in result.stderr.lower()
        
        # å­˜åœ¨ã—ãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        nonexistent_config = temp_dir / "nonexistent.json"
        result = subprocess.run([
            sys.executable, "scripts/start.py",
            "--config", str(nonexistent_config)
        ], capture_output=True, text=True, cwd=project_root)
        
        assert result.returncode != 0
    
    def test_session_persistence_e2e(self, temp_dir):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æ°¸ç¶šåŒ–ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
        config = {
            "version": "1.0.0",
            "application": {"name": "Session Persistence Test"},
            "llm": {
                "default_provider": "openai",
                "providers": {
                    "openai": {
                        "enabled": True,
                        "api_key": "test_key",
                        "models": {
                            "default": "gpt-3.5-turbo",
                            "available": ["gpt-3.5-turbo"]
                        }
                    }
                }
            },
            "storage": {
                "chat_history": {
                    "enabled": True,
                    "storage_path": str(temp_dir / "sessions")
                }
            }
        }
        
        config_path = temp_dir / "session_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        session_dir = temp_dir / "sessions"
        session_dir.mkdir(exist_ok=True)
        
        # ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        test_session = {
            "metadata": {
                "created_at": "2024-01-01T10:00:00",
                "provider": "openai",
                "model": "gpt-3.5-turbo"
            },
            "messages": [
                {
                    "user": "Hello",
                    "assistant": "Hi there!",
                    "timestamp": "2024-01-01T10:00:00"
                }
            ]
        }
        
        session_file = session_dir / "test_session.json"
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(test_session, f)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        from src.ui.cli import CLIInterface
        from src.core.config_manager import ConfigManager
        from src.core.logger import setup_logger
        
        config_manager = ConfigManager()
        config_manager.load_config(str(config_path))
        logger = setup_logger("SessionTest")
        
        cli = CLIInterface(config_manager, logger)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿
        cli.load_session(str(session_file))
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        assert len(cli.current_session) == 1
        assert cli.current_session[0]["user"] == "Hello"
        assert cli.current_session[0]["assistant"] == "Hi there!"


@pytest.mark.integration
class TestSystemStress:
    """ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    @patch('src.llm.openai_provider.OpenAI')
    def test_concurrent_requests(self, mock_openai_class, config_manager, test_logger):
        """ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ"""
        import threading
        import queue
        
        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¢ãƒƒã‚¯
        mock_client = Mock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message = Mock()
        mock_response.choices[0].message.content = "Concurrent response"
        mock_client.chat.completions.create.return_value = mock_response
        mock_openai_class.return_value = mock_client
        
        from src.services.llm_service import LLMService
        llm_service = LLMService(config_manager, test_logger)
        
        # çµæœã‚­ãƒ¥ãƒ¼
        results = queue.Queue()
        
        def send_message_worker(message_id):
            try:
                response = llm_service.send_message(f"Message {message_id}")
                results.put(("success", message_id, response))
            except Exception as e:
                results.put(("error", message_id, str(e)))
        
        # 10å€‹ã®ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        threads = []
        for i in range(10):
            thread = threading.Thread(target=send_message_worker, args=(i,))
            threads.append(thread)
            thread.start()
        
        # ã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
        for thread in threads:
            thread.join(timeout=30)
        
        # çµæœã®ç¢ºèª
        success_count = 0
        error_count = 0
        
        while not results.empty():
            status, message_id, result = results.get()
            if status == "success":
                success_count += 1
                assert isinstance(result, str)
                assert len(result) > 0
            else:
                error_count += 1
        
        # å¤§éƒ¨åˆ†ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæˆåŠŸã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert success_count >= 8  # 80%ä»¥ä¸Šã®æˆåŠŸç‡
    
    def test_memory_leak_detection(self, config_manager, test_logger):
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        import psutil
        import gc
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        from src.services.llm_service import LLMService
        
        # å¤šæ•°ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã¨å‰Šé™¤
        for i in range(100):
            service = LLMService(config_manager, test_logger)
            # ä½¿ç”¨å¾Œã«æ˜ç¤ºçš„ã«å‰Šé™¤
            del service
            
            # å®šæœŸçš„ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            if i % 10 == 0:
                gc.collect()
        
        # æœ€çµ‚çš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # ãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒè¨±å®¹ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert memory_increase < 20, f"Memory increased by {memory_increase}MB"
    
    def test_long_running_session(self, config_manager, test_logger):
        """é•·æ™‚é–“å®Ÿè¡Œã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        from src.ui.cli import CLIInterface
        
        cli = CLIInterface(config_manager, test_logger)
        
        # å¤§é‡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        for i in range(1000):
            cli.current_session.append({
                "user": f"Message {i}",
                "assistant": f"Response {i}",
                "timestamp": f"2024-01-01T{i%24:02d}:00:00"
            })
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ç¢ºèª
        assert len(cli.current_session) == 1000
        assert cli.current_session[0]["user"] == "Message 0"
        assert cli.current_session[999]["user"] == "Message 999"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¢ºèª
        import sys
        session_size = sys.getsizeof(cli.current_session)
        assert session_size < 10 * 1024 * 1024  # 10MBä»¥ä¸‹


@pytest.mark.integration
class TestSystemCompatibility:
    """ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def test_python_version_compatibility(self):
        """Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        import sys
        
        # Python 3.8ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert sys.version_info >= (3, 8), f"Python {sys.version_info} is not supported"
        
        # å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
        required_modules = [
            "json", "os", "sys", "pathlib", "typing",
            "logging", "configparser", "argparse"
        ]
        
        for module_name in required_modules:
            try:
                __import__(module_name)
            except ImportError:
                pytest.fail(f"Required module '{module_name}' is not available")
    
    def test_file_system_compatibility(self, temp_dir):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        # é•·ã„ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ†ã‚¹ãƒˆ
        long_filename = "a" * 200 + ".json"
        long_file_path = temp_dir / long_filename
        
        try:
            with open(long_file_path, 'w') as f:
                f.write('{"test": "data"}')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
            assert long_file_path.exists()
            
        except OSError:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãŒé•·ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„å ´åˆ
            pytest.skip("File system does not support long filenames")
        
        # Unicode ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ†ã‚¹ãƒˆ
        unicode_filename = "ãƒ†ã‚¹ãƒˆ_Ñ„Ğ°Ğ¹Ğ»_ğŸš€.json"
        unicode_file_path = temp_dir / unicode_filename
        
        try:
            with open(unicode_file_path, 'w', encoding='utf-8') as f:
                json.dump({"unicode": "test"}, f, ensure_ascii=False)
            
            assert unicode_file_path.exists()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
            with open(unicode_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                assert data["unicode"] == "test"
                
        except (OSError, UnicodeError):
            pytest.skip("File system does not support Unicode filenames")
    
    def test_environment_variable_handling(self):
        """ç’°å¢ƒå¤‰æ•°å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        import os
        
        # ç’°å¢ƒå¤‰æ•°ã®è¨­å®šã¨å–å¾—
        test_var_name = "LLM_CHAT_TEST_VAR"
        test_var_value = "test_value_123"
        
        # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
        os.environ[test_var_name] = test_var_value
        
        # ç’°å¢ƒå¤‰æ•°ã®å–å¾—ç¢ºèª
        retrieved_value = os.getenv(test_var_name)
        assert retrieved_value == test_var_value
        
        # ç’°å¢ƒå¤‰æ•°ã®å‰Šé™¤
        del os.environ[test_var_name]
        
        # å‰Šé™¤å¾Œã®ç¢ºèª
        assert os.getenv(test_var_name) is None


def run_all_e2e_tests():
    """ã™ã¹ã¦ã®E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    import subprocess
    
    test_commands = [
        # åŸºæœ¬çš„ãªE2Eãƒ†ã‚¹ãƒˆ
        ["python", "-m", "pytest", "tests/test_e2e.py::TestEndToEnd", "-v"],
        
        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
        ["python", "-m", "pytest", "tests/test_e2e.py::TestSystemStress", "-v", "-s"],
        
        # äº’æ›æ€§ãƒ†ã‚¹ãƒˆ
        ["python", "-m", "pytest", "tests/test_e2e.py::TestSystemCompatibility", "-v"]
    ]
    
    all_passed = True
    
    for cmd in test_commands:
        print(f"\nğŸ§ª Running: {' '.join(cmd)}")
        result = subprocess.run(cmd, cwd=project_root)
        
        if result.returncode != 0:
            print(f"âŒ Test failed: {' '.join(cmd)}")
            all_passed = False
        else:
            print(f"âœ… Test passed: {' '.join(cmd)}")
    
    return all_passed


if __name__ == "__main__":
    # ç›´æ¥å®Ÿè¡Œæ™‚ã¯ã™ã¹ã¦ã®E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    success = run_all_e2e_tests()
    
    if success:
        print("\nğŸ‰ All E2E tests passed!")
    else:
        print("\nğŸ’¥ Some E2E tests failed!")
    
    sys.exit(0 if success else 1)

