# tests/generate_report.py
"""
Test Report Generator
ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class TestReportGenerator:
    """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.project_root = project_root
        self.report_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "project_name": "LLM Chat System",
                "version": "1.0.0"
            },
            "system_info": {},
            "test_results": {},
            "coverage_info": {},
            "quality_metrics": {},
            "recommendations": []
        }
    
    def collect_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’åé›†"""
        import platform
        import sys
        
        self.report_data["system_info"] = {
            "platform": platform.platform(),
            "python_version": sys.version,
            "python_executable": sys.executable,
            "working_directory": str(self.project_root),
            "environment_variables": {
                "OPENAI_API_KEY": "SET" if os.getenv("OPENAI_API_KEY") else "NOT_SET",
                "ANTHROPIC_API_KEY": "SET" if os.getenv("ANTHROPIC_API_KEY") else "NOT_SET"
            }
        }
    
    def run_comprehensive_tests(self):
        """åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ§ª Running comprehensive test suite...")
        
        # ãƒ†ã‚¹ãƒˆã‚³ãƒãƒ³ãƒ‰ã®å®šç¾©
        test_commands = {
            "unit_tests": {
                "command": ["python", "-m", "pytest", "tests/", "-v", 
                           "-m", "not integration and not slow", 
                           "--cov=src", "--cov-report=json"],
                "description": "Unit tests with coverage"
            },
            "integration_tests": {
                "command": ["python", "-m", "pytest", "tests/test_integration.py", 
                           "-v", "-m", "integration"],
                "description": "Integration tests"
            },
            "linting": {
                "command": ["python", "-m", "flake8", "src/", "tests/", "scripts/",
                           "--max-line-length=88", "--extend-ignore=E203,W503"],
                "description": "Code style checking"
            },
            "security": {
                "command": ["python", "-m", "bandit", "-r", "src/", "-f", "json"],
                "description": "Security vulnerability scanning"
            }
        }
        
        # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        for test_name, test_config in test_commands.items():
            print(f"\nğŸ“‹ Running {test_config['description']}...")
            
            start_time = time.time()
            result = subprocess.run(
                test_config["command"],
                cwd=self.project_root,
                capture_output=True,
                text=True
            )
            end_time = time.time()
            
            self.report_data["test_results"][test_name] = {
                "success": result.returncode == 0,
                "execution_time": end_time - start_time,
                "command": " ".join(test_config["command"]),
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode
            }
            
            status = "âœ… PASS" if result.returncode == 0 else "âŒ FAIL"
            print(f"{status} {test_name} ({end_time - start_time:.2f}s)")
    
    def collect_coverage_info(self):
        """ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±ã‚’åé›†"""
        coverage_file = self.project_root / "coverage.json"
        
        if coverage_file.exists():
            try:
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                
                self.report_data["coverage_info"] = {
                    "total_coverage": coverage_data.get("totals", {}).get("percent_covered", 0),
                    "files": coverage_data.get("files", {}),
                    "summary": coverage_data.get("totals", {})
                }
            except Exception as e:
                self.report_data["coverage_info"] = {
                    "error": f"Failed to parse coverage data: {e}"
                }
        else:
            self.report_data["coverage_info"] = {
                "message": "Coverage data not available"
            }
    
    def analyze_code_quality(self):
        """ã‚³ãƒ¼ãƒ‰å“è³ªã‚’åˆ†æ"""
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã¨ã‚³ãƒ¼ãƒ‰è¡Œæ•°ã®è¨ˆç®—
        src_files = list((self.project_root / "src").rglob("*.py"))
        test_files = list((self.project_root / "tests").rglob("*.py"))
        
        total_lines = 0
        total_files = len(src_files)
        
        for file_path in src_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = len(f.readlines())
                    total_lines += lines
            except Exception:
                continue
        
        self.report_data["quality_metrics"] = {
            "source_files": total_files,
            "total_lines_of_code": total_lines,
            "test_files": len(test_files),
            "average_lines_per_file": total_lines / total_files if total_files > 0 else 0,
            "test_to_source_ratio": len(test_files) / total_files if total_files > 0 else 0
        }
    
    def generate_recommendations(self):
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        test_results = self.report_data["test_results"]
        
        if not test_results.get("unit_tests", {}).get("success", False):
            recommendations.append({
                "type": "critical",
                "title": "Unit Tests Failing",
                "description": "Unit tests are failing. Fix failing tests before deployment.",
                "priority": "high"
            })
        
        if not test_results.get("integration_tests", {}).get("success", False):
            recommendations.append({
                "type": "warning",
                "title": "Integration Tests Failing",
                "description": "Integration tests are failing. Check system integration.",
                "priority": "medium"
            })
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã«åŸºã¥ãæ¨å¥¨äº‹é …
        coverage_info = self.report_data["coverage_info"]
        if isinstance(coverage_info.get("total_coverage"), (int, float)):
            coverage = coverage_info["total_coverage"]
            if coverage < 80:
                recommendations.append({
                    "type": "improvement",
                    "title": "Low Test Coverage",
                    "description": f"Test coverage is {coverage:.1f}%. Aim for at least 80%.",
                    "priority": "medium"
                })
            elif coverage >= 90:
                recommendations.append({
                    "type": "success",
                    "title": "Excellent Test Coverage",
                    "description": f"Test coverage is {coverage:.1f}%. Great job!",
                    "priority": "low"
                })
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªã«åŸºã¥ãæ¨å¥¨äº‹é …
        quality_metrics = self.report_data["quality_metrics"]
        avg_lines = quality_metrics.get("average_lines_per_file", 0)
        
        if avg_lines > 300:
            recommendations.append({
                "type": "improvement",
                "title": "Large Files Detected",
                "description": f"Average file size is {avg_lines:.0f} lines. Consider breaking down large files.",
                "priority": "low"
            })
        
        test_ratio = quality_metrics.get("test_to_source_ratio", 0)
        if test_ratio < 0.5:
            recommendations.append({
                "type": "improvement",
                "title": "Low Test File Ratio",
                "description": f"Test-to-source file ratio is {test_ratio:.2f}. Consider adding more test files.",
                "priority": "medium"
            })
        
        # ç’°å¢ƒã«åŸºã¥ãæ¨å¥¨äº‹é …
        env_vars = self.report_data["system_info"]["environment_variables"]
        if env_vars.get("OPENAI_API_KEY") == "NOT_SET":
            recommendations.append({
                "type": "warning",
                "title": "Missing API Key",
                "description": "OPENAI_API_KEY is not set. Some features may not work.",
                "priority": "medium"
            })
        
        self.report_data["recommendations"] = recommendations
    
    def generate_html_report(self, output_path: str):
        """HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Chat System - Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }
        .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }
        .section { margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }
        .success { background-color: #d4edda; border-color: #c3e6cb; }
        .warning { background-color: #fff3cd; border-color: #ffeaa7; }
        .error { background-color: #f8d7da; border-color: #f5c6cb; }
        .metric { display: inline-block; margin: 10px; padding: 10px; background: #f8f9fa; border-radius: 3px; }
        .recommendation { margin: 10px 0; padding: 10px; border-left: 4px solid #007bff; background: #f8f9fa; }
        .critical { border-left-color: #dc3545; }
        .warning-rec { border-left-color: #ffc107; }
        .improvement { border-left-color: #17a2b8; }
        .success-rec { border-left-color: #28a745; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }
        th { background-color: #f2f2f2; }
        .status-pass { color: #28a745; font-weight: bold; }
        .status-fail { color: #dc3545; font-weight: bold; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ LLM Chat System - Test Report</h1>
        <p>Generated on: {generated_at}</p>
        <p>Project Version: {version}</p>
    </div>

    <div class="section">
        <h2>ğŸ“Š Test Results Summary</h2>
        <div class="metric">
            <strong>Total Test Suites:</strong> {total_suites}
        </div>
        <div class="metric">
            <strong>Passed:</strong> <span class="status-pass">{passed_suites}</span>
        </div>
        <div class="metric">
            <strong>Failed:</strong> <span class="status-fail">{failed_suites}</span>
        </div>
        <div class="metric">
            <strong>Success Rate:</strong> {success_rate:.1f}%
        </div>
    </div>

    <div class="section">
        <h2>ğŸ§ª Detailed Test Results</h2>
        <table>
            <thead>
                <tr>
                    <th>Test Suite</th>
                    <th>Status</th>
                    <th>Execution Time</th>
                    <th>Details</th>
                </tr>
            </thead>
            <tbody>
                {test_rows}
            </tbody>
        </table>
    </div>

    <div class="section">
        <h2>ğŸ“ˆ Coverage Information</h2>
        {coverage_section}
    </div>

    <div class="section">
        <h2>ğŸ“ Quality Metrics</h2>
        <div class="metric">
            <strong>Source Files:</strong> {source_files}
        </div>
        <div class="metric">
            <strong>Test Files:</strong> {test_files}
        </div>
        <div class="metric">
            <strong>Lines of Code:</strong> {total_lines}
        </div>
        <div class="metric">
            <strong>Test Coverage:</strong> {coverage_percent}%
        </div>
    </div>

    <div class="section">
        <h2>ğŸ’¡ Recommendations</h2>
        {recommendations_section}
    </div>

    <div class="section">
        <h2>ğŸ–¥ï¸ System Information</h2>
        <p><strong>Platform:</strong> {platform}</p>
        <p><strong>Python Version:</strong> {python_version}</p>
        <p><strong>Working Directory:</strong> {working_directory}</p>
    </div>
</body>
</html>
        """
        
        # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        test_results = self.report_data["test_results"]
        total_suites = len(test_results)
        passed_suites = sum(1 for result in test_results.values() if result.get("success", False))
        failed_suites = total_suites - passed_suites
        success_rate = (passed_suites / total_suites * 100) if total_suites > 0 else 0
        
        # ãƒ†ã‚¹ãƒˆçµæœãƒ†ãƒ¼ãƒ–ãƒ«ã®ç”Ÿæˆ
        test_rows = ""
        for test_name, result in test_results.items():
            status = "PASS" if result.get("success", False) else "FAIL"
            status_class = "status-pass" if result.get("success", False) else "status-fail"
            execution_time = f"{result.get('execution_time', 0):.2f}s"
            
            test_rows += f"""
                <tr>
                    <td>{test_name.replace('_', ' ').title()}</td>
                    <td><span class="{status_class}">{status}</span></td>
                    <td>{execution_time}</td>
                    <td>Return Code: {result.get('return_code', 'N/A')}</td>
                </tr>
            """
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆ
        coverage_info = self.report_data["coverage_info"]
        if "total_coverage" in coverage_info:
            coverage_section = f"""
                <div class="metric">
                    <strong>Total Coverage:</strong> {coverage_info['total_coverage']:.1f}%
                </div>
            """
        else:
            coverage_section = "<p>Coverage information not available</p>"
        
        # æ¨å¥¨äº‹é …ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆ
        recommendations_section = ""
        for rec in self.report_data["recommendations"]:
            rec_class = {
                "critical": "critical",
                "warning": "warning-rec",
                "improvement": "improvement",
                "success": "success-rec"
            }.get(rec["type"], "improvement")
            
            recommendations_section += f"""
                <div class="recommendation {rec_class}">
                    <h4>{rec['title']}</h4>
                    <p>{rec['description']}</p>
                    <small>Priority: {rec['priority']}</small>
                </div>
            """
        
        # HTMLã®ç”Ÿæˆ
        html_content = html_template.format(
            generated_at=self.report_data["metadata"]["generated_at"],
            version=self.report_data["metadata"]["version"],
            total_suites=total_suites,
            passed_suites=passed_suites,
            failed_suites=failed_suites,
            success_rate=success_rate,
            test_rows=test_rows,
            coverage_section=coverage_section,
            coverage_percent=coverage_info.get("total_coverage", 0),
            source_files=self.report_data["quality_metrics"].get("source_files", 0),
            test_files=self.report_data["quality_metrics"].get("test_files", 0),
            total_lines=self.report_data["quality_metrics"].get("total_lines_of_code", 0),
            recommendations_section=recommendations_section,
            platform=self.report_data["system_info"]["platform"],
            python_version=self.report_data["system_info"]["python_version"],
            working_directory=self.report_data["system_info"]["working_directory"]
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
    
    def generate_full_report(self):
        """å®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ“Š Generating comprehensive test report...")
        
        # 1. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±åé›†
        self.collect_system_info()
        
        # 2. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        self.run_comprehensive_tests()
        
        # 3. ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±åé›†
        self.collect_coverage_info()
        
        # 4. ã‚³ãƒ¼ãƒ‰å“è³ªåˆ†æ
        self.analyze_code_quality()
        
        # 5. æ¨å¥¨äº‹é …ç”Ÿæˆ
        self.generate_recommendations()
        
        # 6. ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ãƒ¬ãƒãƒ¼ãƒˆ
        json_path = self.project_root / f"test_report_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, indent=2, ensure_ascii=False)
        
        # HTML ãƒ¬ãƒãƒ¼ãƒˆ
        html_path = self.project_root / f"test_report_{timestamp}.html"
        self.generate_html_report(str(html_path))
        
        print(f"âœ… Reports generated:")
        print(f"   ğŸ“„ JSON: {json_path}")
        print(f"   ğŸŒ HTML: {html_path}")
        
        return self.report_data


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    generator = TestReportGenerator()
    report_data = generator.generate_full_report()
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    test_results = report_data["test_results"]
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results.values() if result.get("success", False))
    
    print(f"\nğŸ¯ Test Summary:")
    print(f"   Total Test Suites: {total_tests}")
    print(f"   Passed: {passed_tests}")
    print(f"   Failed: {total_tests - passed_tests}")
    print(f"   Success Rate: {(passed_tests/total_tests*100):.1f}%")
    
    # æ¨å¥¨äº‹é …è¡¨ç¤º
    recommendations = report_data["recommendations"]
    if recommendations:
        print(f"\nğŸ’¡ Key Recommendations:")
        for rec in recommendations[:3]:  # ä¸Šä½3ã¤ã‚’è¡¨ç¤º
            print(f"   â€¢ {rec['title']}: {rec['description']}")
    
    return passed_tests == total_tests


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
