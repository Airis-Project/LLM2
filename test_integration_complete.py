# åŒ…æ‹¬çš„çµ±åˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# å®Ÿè¡Œæ¨©é™ä»˜ä¸
#chmod +x test_integration_complete.py

# åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
#python test_integration_complete.py

"""
å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆ - å…¨æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
"""

import sys
import time
import asyncio
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, str(Path(__file__).parent))

from src.llm_client_v2 import EnhancedLLMClient
from src.model_selector import TaskType

def print_section(title: str):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {title}")
    print('='*60)

def print_test(name: str, prompt: str):
    """ãƒ†ã‚¹ãƒˆé–‹å§‹è¡¨ç¤º"""
    print(f"\nğŸ“ ãƒ†ã‚¹ãƒˆ: {name}")
    print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt[:50]}{'...' if len(prompt) > 50 else ''}")

async def run_comprehensive_tests():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    print_section("LLM Code Assistant å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆ")
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = EnhancedLLMClient()
    
    print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {len(client.get_available_models())}å€‹")
    for model in client.get_available_models()[:5]:
        print(f"   â€¢ {model}")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
    test_cases = [
        # åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        {
            "name": "Pythoné–¢æ•°ç”Ÿæˆ",
            "prompt": "Create a Python function to calculate factorial",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "quality"
        },
        {
            "name": "JavaScripté–¢æ•°ç”Ÿæˆ", 
            "prompt": "Write a JavaScript function to validate email",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "balanced"
        },
        {
            "name": "SQL ã‚¯ã‚¨ãƒªç”Ÿæˆ",
            "prompt": "Write SQL to find top 5 customers by purchase amount",
            "task_type": TaskType.CODE_GENERATION,
            "priority": "quality"
        },
        
        # ã‚³ãƒ¼ãƒ‰èª¬æ˜ãƒ†ã‚¹ãƒˆ
        {
            "name": "è¤‡é›‘ãªã‚³ãƒ¼ãƒ‰èª¬æ˜",
            "prompt": "Explain this code: def quicksort(arr): if len(arr) <= 1: return arr; pivot = arr[len(arr)//2]; left = [x for x in arr if x < pivot]; middle = [x for x in arr if x == pivot]; right = [x for x in arr if x > pivot]; return quicksort(left) + middle + quicksort(right)",
            "task_type": TaskType.CODE_EXPLANATION,
            "priority": "quality"
        },
        {
            "name": "æ­£è¦è¡¨ç¾èª¬æ˜",
            "prompt": "Explain this regex: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$",
            "task_type": TaskType.CODE_EXPLANATION,
            "priority": "balanced"
        },
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¹ãƒˆ
        {
            "name": "Pythonæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "prompt": "Fix this Python code: for i in range(10) print(i)",
            "task_type": TaskType.DEBUGGING,
            "priority": "speed"
        },
        {
            "name": "è«–ç†ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "prompt": "Fix this code: def is_even(n): return n % 2 == 1",
            "task_type": TaskType.DEBUGGING,
            "priority": "balanced"
        },
        {
            "name": "JavaScript ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "prompt": "Fix this JS: function add(a, b) { return a + b; } console.log(add(1));",
            "task_type": TaskType.DEBUGGING,
            "priority": "balanced"
        },
        
        # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚³ãƒ¼ãƒ‰æœ€é©åŒ–",
            "prompt": "Refactor this code for better performance: def find_max(arr): max_val = arr[0]; for i in range(1, len(arr)): if arr[i] > max_val: max_val = arr[i]; return max_val",
            "task_type": TaskType.REFACTORING,
            "priority": "quality"
        },
        {
            "name": "å¯èª­æ€§å‘ä¸Š",
            "prompt": "Make this code more readable: x=[i for i in range(100) if i%2==0 and i%3==0 and i>10]",
            "task_type": TaskType.REFACTORING,
            "priority": "balanced"
        },
        
        # é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚¯ã‚¤ãƒƒã‚¯è£œå®Œ1",
            "prompt": "Complete: def hello_world():",
            "task_type": TaskType.QUICK_RESPONSE,
            "priority": "speed"
        },
        {
            "name": "ã‚¯ã‚¤ãƒƒã‚¯è£œå®Œ2", 
            "prompt": "Complete: import pandas as pd; df = pd.read_csv('data.csv'); df.",
            "task_type": TaskType.QUICK_RESPONSE,
            "priority": "speed"
        },
        
        # è¤‡é›‘åˆ†æãƒ†ã‚¹ãƒˆ
        {
            "name": "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ†æ",
            "prompt": "Analyze the time and space complexity of merge sort algorithm",
            "task_type": TaskType.COMPLEX_ANALYSIS,
            "priority": "quality"
        },
        {
            "name": "è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³èª¬æ˜",
            "prompt": "Explain the Singleton design pattern with Python example",
            "task_type": TaskType.COMPLEX_ANALYSIS,
            "priority": "quality"
        }
    ]
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = []
    total_start_time = time.time()
    
    for i, test_case in enumerate(test_cases, 1):
        print_test(f"{i}. {test_case['name']}", test_case['prompt'])
        
        start_time = time.time()
        try:
            response = await client.generate_async(
                prompt=test_case['prompt'],
                task_type=test_case['task_type'],
                priority=test_case['priority']
            )
            
            duration = time.time() - start_time
            
            if response['success']:
                print(f"   âœ… æˆåŠŸ ({duration:.1f}s)")
                print(f"   ğŸ¤– ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {response['model']}")
                print(f"   ğŸ“ å¿œç­”: {response['content'][:100]}{'...' if len(response['content']) > 100 else ''}")
                
                results.append({
                    'name': test_case['name'],
                    'success': True,
                    'duration': duration,
                    'model': response['model'],
                    'task_type': test_case['task_type'].value,
                    'priority': test_case['priority']
                })
            else:
                print(f"   âŒ å¤±æ•—: {response['error']}")
                results.append({
                    'name': test_case['name'],
                    'success': False,
                    'duration': duration,
                    'error': response['error'],
                    'task_type': test_case['task_type'].value,
                    'priority': test_case['priority']
                })
                
        except Exception as e:
            duration = time.time() - start_time
            print(f"   âŒ ä¾‹å¤–: {e}")
            results.append({
                'name': test_case['name'],
                'success': False,
                'duration': duration,
                'error': str(e),
                'task_type': test_case['task_type'].value,
                'priority': test_case['priority']
            })
    
    # çµæœåˆ†æ
    total_duration = time.time() - total_start_time
    successful_tests = [r for r in results if r['success']]
    failed_tests = [r for r in results if not r['success']]
    
    print_section("å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆçµæœ")
    
    print(f"ğŸ“Š ç·åˆçµæœ:")
    print(f"   æˆåŠŸç‡: {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.1f}%)")
    print(f"   ç·å®Ÿè¡Œæ™‚é–“: {total_duration:.1f}ç§’")
    print(f"   å¹³å‡å¿œç­”æ™‚é–“: {sum(r['duration'] for r in successful_tests)/len(successful_tests):.1f}ç§’" if successful_tests else "   å¹³å‡å¿œç­”æ™‚é–“: N/A")
    
    # æˆåŠŸãƒ†ã‚¹ãƒˆè©³ç´°
    if successful_tests:
        print(f"\nâœ… æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ ({len(successful_tests)}å€‹):")
        for result in successful_tests:
            print(f"   â€¢ {result['name']}: {result['duration']:.1f}s ({result['model']})")
    
    # å¤±æ•—ãƒ†ã‚¹ãƒˆè©³ç´°
    if failed_tests:
        print(f"\nâŒ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ ({len(failed_tests)}å€‹):")
        for result in failed_tests:
            print(f"   â€¢ {result['name']}: {result.get('error', 'Unknown error')}")
    
    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥åˆ†æ
    print(f"\nğŸ“ˆ ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥æˆåŠŸç‡:")
    task_types = {}
    for result in results:
        task_type = result['task_type']
        if task_type not in task_types:
            task_types[task_type] = {'total': 0, 'success': 0}
        task_types[task_type]['total'] += 1
        if result['success']:
            task_types[task_type]['success'] += 1
    
    for task_type, stats in task_types.items():
        success_rate = stats['success'] / stats['total'] * 100
        print(f"   â€¢ {task_type}: {stats['success']}/{stats['total']} ({success_rate:.1f}%)")
    
    # å„ªå…ˆåº¦åˆ¥åˆ†æ
    print(f"\nâš¡ å„ªå…ˆåº¦åˆ¥å¹³å‡å¿œç­”æ™‚é–“:")
    priorities = {}
    for result in successful_tests:
        priority = result['priority']
        if priority not in priorities:
            priorities[priority] = []
        priorities[priority].append(result['duration'])
    
    for priority, durations in priorities.items():
        avg_duration = sum(durations) / len(durations)
        print(f"   â€¢ {priority}: {avg_duration:.1f}ç§’ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(durations)})")
    
    # ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çµ±è¨ˆ
    print(f"\nğŸ¤– ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çµ±è¨ˆ:")
    model_usage = {}
    for result in successful_tests:
        model = result['model']
        model_usage[model] = model_usage.get(model, 0) + 1
    
    for model, count in sorted(model_usage.items(), key=lambda x: x[1], reverse=True):
        print(f"   â€¢ {model}: {count}å›")
    
    print_section("çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
    
    return {
        'total_tests': len(results),
        'successful_tests': len(successful_tests),
        'success_rate': len(successful_tests) / len(results) * 100,
        'total_duration': total_duration,
        'results': results
    }

if __name__ == "__main__":
    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(run_comprehensive_tests())

